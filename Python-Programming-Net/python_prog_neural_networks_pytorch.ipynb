{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythonprogramming.net/training-deep-learning-neural-network-pytorch/\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # similar as nn, most times both would work\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                      transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                      transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing batch size decreases training time, but your computer might have limitations\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "test = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 1, 7, 5, 8, 7, 3, 8, 4, 9])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# data[0] is a bunch of features, data[1] is the labels\n",
    "# e.g. if data[1][0] is a 2 label, data[0][0]'s features are a 2, data[0][0] is 1 x 28 x 28 as seen below\n",
    "x, y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d594c35948>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(data[0][0].shape) # returns torch.Size([1, 28, 28]). Since this has a 1 in front, need to reshape\n",
    "# plt.imshow(data[0][0]) # just using this would return Invalid shape (1, 28, 28) for image data\n",
    "\n",
    "plt.imshow(data[0][0].view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "# checking if dataset is balanced and has equal number of features for all labels\n",
    "total = 0 \n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i] / total * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  #???\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)  # flattened 28 * 28 img size for input, any number for output going to next layer\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)  # classify into 10 classes (0 to 9)\n",
    "    \n",
    "    # now we want to define how the data passes through, and which layer is first\n",
    "    # sidenote: if you want to include logic, you can add that here as well\n",
    "    def forward(self, x):\n",
    "        # for hidden layers, want relu to determine if neuron \"fires\" or not\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # for last layer, want a probability distribution, then can take the class w/ highest probability\n",
    "        x = self.fc4(x)\n",
    "        # dim=1 grabbing the output values\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3461, -2.2938, -2.2629, -2.3187, -2.3039, -2.2898, -2.3865, -2.3343,\n",
       "         -2.2424, -2.2561]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "# print(net)\n",
    "X = torch.rand((28, 28))\n",
    "# X = X.view(28 * 28) # will give you Dimension out of range (expected to be in range of [-1, 0], but got 1) error\n",
    "X = X.view(-1, 28 * 28)  # -1 or 1 specifies input is of unknown shape\n",
    "output = net(X)\n",
    "# this shows that we can successfully pass through data in our NN class even w/o any grad descent optimisation etc\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# can tell model to not adjust \"well optimised\" earlier layer weights, and only adjust later layers\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # data is a batch of featuresets and labels\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28 * 28))  # return a 1x10 tensor that has probability for each label\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()  # backprop\n",
    "        optimizer.step()  \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calc accuracy and double check w/ sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.972\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# disables gradient calc https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
    "# don't want to calc gradient here, just testing how good model is\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        # argmax returns indice of largest value\n",
    "        # since indices are ordered by label from 0 to 9, that also returns the predicted label\n",
    "        # compare predicted against actual value in the idx-th entry of y e.g. 0th entry, 1st entry\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "print(\"Accuracy: \", round(correct / total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[0].view(28, 28))\n",
    "plt.show()  # visualise feature/label\n",
    "print(torch.argmax(net(X[0].view(-1, 28 * 28))[0]))  # check what label was"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
